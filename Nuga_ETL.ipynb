{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyspark in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.5.3)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.0.36)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (17.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark sqlalchemy pandas pyarrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('NugaBankETL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-HN0EAOS6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NugaBankETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23e14989100>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing spark session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nuga_bank_df = spark.read.csv(r'dataset\\nuga_bank_transactions.csv', header= True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|    Transaction_Date|Amount|Transaction_Type| Customer_Name|    Customer_Address|     Customer_City|Customer_State|    Customer_Country|             Company|           Job_Title|               Email|       Phone_Number|Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|Group|Is_Active|        Last_Updated|         Description|Gender|Marital_Status|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|2024-03-23 15:38:...| 34.76|      Withdrawal|    James Neal|54912 Holmes Lodg...| West Keithborough|       Florida|                Togo|Benson, Johnson a...|                NULL|                NULL|  493.720.6609x7545|  3592901394693441|GB98RBPP090285271...|          MAD|       3167.0|       C|    Z|       No|2020-06-20 03:04:...|Yeah food anythin...| Other|      Divorced|\n",
      "|2024-04-22 19:15:...|163.92|      Withdrawal|   Thomas Long| 1133 Collin Passage|        Joshuabury|   Connecticut|Lao People's Demo...|                NULL|   Food technologist|michellelynch@exa...|      (497)554-3317|              NULL|GB03KFZR339662263...|          VEF|       2122.0|       B|    Z|     NULL|2020-12-27 13:23:...|Teach edge make n...|Female|       Married|\n",
      "|2024-04-12 19:46:...|386.32|      Withdrawal|Ashley Shelton|5297 Johnson Port...|       North Maria|    New Jersey|              Bhutan|       Jones-Mueller|Database administ...| ljordan@example.org|      (534)769-3072|      675983949974|GB59QYRN446730519...|          COP|       7796.0|       C|    Z|       No|2020-01-24 01:23:...|Again line face c...| Other|          NULL|\n",
      "|2024-04-17 15:29:...|407.15|         Deposit| James Rosario|56955 Moore Glens...|North Michellefurt|    New Mexico|             Iceland|       Vargas-Harris|Horticultural the...|parkerjames@examp...|+1-447-900-1320x257|     4761202519057|GB74FTDO268299438...|          BWP|       6284.0|       C|    Z|      Yes|2023-09-27 03:01:...|     Bag my a drive.|  NULL|          NULL|\n",
      "|2024-02-10 01:51:...|161.31|         Deposit|Miguel Leonard|262 Beck Expressw...|              NULL| West Virginia|             Eritrea|Richardson, Gonza...|   Minerals surveyor| zweaver@example.net|               NULL|   213156729655186|GB94EWRN587847592...|          SOS|       9179.0|       C|    Y|       No|2022-01-22 19:08:...|Husband find ok w...|Female|       Married|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Nuga_bank_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Date: timestamp (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Transaction_Type: string (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Customer_Address: string (nullable = true)\n",
      " |-- Customer_City: string (nullable = true)\n",
      " |-- Customer_State: string (nullable = true)\n",
      " |-- Customer_Country: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Job_Title: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone_Number: string (nullable = true)\n",
      " |-- Credit_Card_Number: long (nullable = true)\n",
      " |-- IBAN: string (nullable = true)\n",
      " |-- Currency_Code: string (nullable = true)\n",
      " |-- Random_Number: double (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Group: string (nullable = true)\n",
      " |-- Is_Active: string (nullable = true)\n",
      " |-- Last_Updated: timestamp (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Nuga_bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To determine the number of rows\n",
    "no_of_rows =Nuga_bank_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to determine the number of columns\n",
    "no_of_columns = len(Nuga_bank_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Null 0\n",
      "Amount Null 0\n",
      "Transaction_Type Null 0\n",
      "Customer_Name Null 100425\n",
      "Customer_Address Null 100087\n",
      "Customer_City Null 100034\n",
      "Customer_State Null 100009\n",
      "Customer_Country Null 100672\n",
      "Company Null 100295\n",
      "Job_Title Null 99924\n",
      "Email Null 100043\n",
      "Phone_Number Null 100524\n",
      "Credit_Card_Number Null 100085\n",
      "IBAN Null 100300\n",
      "Currency_Code Null 99342\n",
      "Random_Number Null 99913\n",
      "Category Null 100332\n",
      "Group Null 100209\n",
      "Is_Active Null 100259\n",
      "Last_Updated Null 100321\n",
      "Description Null 100403\n",
      "Gender Null 99767\n",
      "Marital_Status Null 99904\n"
     ]
    }
   ],
   "source": [
    "#checking for null values\n",
    "for column in Nuga_bank_df.columns:\n",
    "    print(column, 'Null', Nuga_bank_df.filter(Nuga_bank_df[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIlling up missing values\n",
    "\n",
    "nuga_bank = Nuga_bank_df.fillna({\n",
    "    'Customer_Name': 'Unknown',\n",
    "    'Customer_Address':'Unknown',\n",
    "    'Customer_City':'Unknown',\n",
    "    'Customer_State':'Unknown',\n",
    "    'Customer_Country':'Unknown',\n",
    "    'Company':'Unknown',\n",
    "    'Job_Title':'Unknown',\n",
    "    'Email': 'Uknown',\n",
    "    'Phone_Number':'Unknown',\n",
    "    'Credit_Card_Number':0,\n",
    "    'IBAN':'Unknown',\n",
    "    'Currency_Code':'Unknown',\n",
    "    'Random_Number':0.0,\n",
    "    'Category':'Unknown',\n",
    "    'Group':'Unknown',\n",
    "    'Is_Active':'Unknown',\n",
    "    'Description':'Unknown',\n",
    "    'Gender':'Unknown',\n",
    "    'Marital_Status':'Unknown',\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Null 0\n",
      "Amount Null 0\n",
      "Transaction_Type Null 0\n",
      "Customer_Name Null 0\n",
      "Customer_Address Null 0\n",
      "Customer_City Null 0\n",
      "Customer_State Null 0\n",
      "Customer_Country Null 0\n",
      "Company Null 0\n",
      "Job_Title Null 0\n",
      "Email Null 0\n",
      "Phone_Number Null 0\n",
      "Credit_Card_Number Null 0\n",
      "IBAN Null 0\n",
      "Currency_Code Null 0\n",
      "Random_Number Null 0\n",
      "Category Null 0\n",
      "Group Null 0\n",
      "Is_Active Null 0\n",
      "Last_Updated Null 100321\n",
      "Description Null 0\n",
      "Gender Null 0\n",
      "Marital_Status Null 0\n"
     ]
    }
   ],
   "source": [
    "for column in nuga_bank.columns:\n",
    "    print(column, 'Null', nuga_bank.filter(nuga_bank[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows where last updated is null\n",
    "nuga_bank = nuga_bank.na.drop(subset=['Last_Updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Nulls 0\n",
      "Amount Nulls 0\n",
      "Transaction_Type Nulls 0\n",
      "Customer_Name Nulls 0\n",
      "Customer_Address Nulls 0\n",
      "Customer_City Nulls 0\n",
      "Customer_State Nulls 0\n",
      "Customer_Country Nulls 0\n",
      "Company Nulls 0\n",
      "Job_Title Nulls 0\n",
      "Email Nulls 0\n",
      "Phone_Number Nulls 0\n",
      "Credit_Card_Number Nulls 0\n",
      "IBAN Nulls 0\n",
      "Currency_Code Nulls 0\n",
      "Random_Number Nulls 0\n",
      "Category Nulls 0\n",
      "Group Nulls 0\n",
      "Is_Active Nulls 0\n",
      "Last_Updated Nulls 0\n",
      "Description Nulls 0\n",
      "Gender Nulls 0\n",
      "Marital_Status Nulls 0\n"
     ]
    }
   ],
   "source": [
    "for column in nuga_bank.columns:\n",
    "    print(column, 'Nulls', nuga_bank.filter(nuga_bank[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899679"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_row = nuga_bank.count()\n",
    "no_of_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+-----------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "|summary|            Amount|Transaction_Type|Customer_Name|    Customer_Address|Customer_City|Customer_State|Customer_Country|      Company|         Job_Title|              Email|        Phone_Number|  Credit_Card_Number|                IBAN|Currency_Code|    Random_Number|Category|  Group|Is_Active|         Description| Gender|Marital_Status|\n",
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+-----------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "|  count|            899679|          899679|       899679|              899679|       899679|        899679|          899679|       899679|            899679|             899679|              899679|              899679|              899679|       899679|           899679|  899679| 899679|   899679|              899679| 899679|        899679|\n",
      "|   mean|505.10367708927123|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL| 6.002851385999521E9|3.409662226750533...|                NULL|         NULL|4952.920380491264|    NULL|   NULL|     NULL|                NULL|   NULL|          NULL|\n",
      "| stddev| 285.7945343300485|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL|2.3068371038619423E9|1.189655005733000...|                NULL|         NULL|2966.543401151591|    NULL|   NULL|     NULL|                NULL|   NULL|          NULL|\n",
      "|    min|              10.0|         Deposit| Aaron Abbott|000 Aaron Landing...|    Aaronberg|       Alabama|     Afghanistan| Abbott Group|Academic librarian|             Uknown|       (200)201-4254|                   0|GB02AAAU191993009...|          AED|              0.0|       A|Unknown|       No|A American and to...| Female|      Divorced|\n",
      "|    max|            1000.0|      Withdrawal|    Zoe Young|             Unknown|  Zunigaville|       Wyoming|        Zimbabwe|Zuniga-Wilson|      Youth worker|zzuniga@example.org|             Unknown| 4999984361512569455|             Unknown|          ZWD|           9999.0| Unknown|      Z|      Yes|Yourself young ev...|Unknown|       Unknown|\n",
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+-----------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summary stats of the data\n",
    "nuga_bank.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-HN0EAOS6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NugaBankETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23e14989100>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling the data/ Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transactions schema\n",
    "Transaction = nuga_bank.select('Transaction_Date', 'Amount', 'Transaction_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a new column to the transaction schema\n",
    "Transaction = Transaction.withColumn('Transaction_ID', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-orfering of the transaction schema\n",
    "Transaction = Transaction.select('Transaction_ID','Transaction_Date', 'Amount', 'Transaction_Type').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+----------------+\n",
      "|Transaction_ID|    Transaction_Date|Amount|Transaction_Type|\n",
      "+--------------+--------------------+------+----------------+\n",
      "|           108|2024-04-30 03:13:...|302.59|         Deposit|\n",
      "|           442|2024-04-27 07:39:...|608.58|         Deposit|\n",
      "|          1019|2024-03-19 12:09:...|951.23|        Transfer|\n",
      "|          1026|2024-02-24 00:22:...|627.81|         Deposit|\n",
      "|          1206|2024-01-16 09:15:...|432.81|        Transfer|\n",
      "+--------------+--------------------+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Transaction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customer table\n",
    "Customer = nuga_bank.select('Customer_Name', 'Customer_Address', 'Customer_City', 'Customer_Country', 'Email', 'Phone_Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a customer_id column:\n",
    "Customer = Customer.withColumn('Customer_ID', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-ordering the customer schema\n",
    "Customer = Customer.select('Customer_ID','Customer_Name', 'Customer_Address', 'Customer_City', 'Customer_Country', 'Email', 'Phone_Number').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|Customer_ID|      Customer_Name|    Customer_Address|   Customer_City|    Customer_Country|               Email|        Phone_Number|\n",
      "+-----------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|       1160|         Amy Malone|   989 Moyer Estates|        Gillland|             Eritrea| scott93@example.net|001-862-947-4500x421|\n",
      "|       1193|      Andrew Benson|  1631 Hawkins Ports|     Lamberttown|United Arab Emirates|duncanbarbara@exa...| +1-539-932-5174x292|\n",
      "|       1235|     Cassandra Rich|    4143 Flores Camp|      Lake Nancy|        Cook Islands|qbarnett@example.net|       (845)729-0291|\n",
      "|       1303|    Colin Fernandez|   40328 Bruce Ridge|     Melissaland|    Christmas Island|griffinandrea@exa...|        637.360.7815|\n",
      "|       1372|Christopher Mercado|     796 Tina Divide|Lake Benjaminton|        Turkmenistan|              Uknown| +1-527-552-0806x036|\n",
      "|       1605|     Mark Alexander|   54362 Scott Trace|     Timothyland|            Thailand|broberts@example.org|   633-266-8264x2966|\n",
      "|       1670|  Charles Alexander|7305 Allison Divi...|      Larsonport|            Portugal| pnewman@example.com|          6105501943|\n",
      "|       2133|         James Hill|    85557 Tina Wells|   Lake Markland|  Russian Federation|whitejade@example...|     +1-811-323-0557|\n",
      "|       2199|    Patricia Jensen| 198 Nathaniel Locks|      Nortontown|             Tunisia|abigail30@example...|  (827)201-0637x6975|\n",
      "|       2256|      Marc Thornton|    227 Keller Plaza| Port Tylermouth|              Guyana|  rsmith@example.org|   483.564.6001x1488|\n",
      "|       2294|     Andrea Cochran|658 Wanda Locks A...|       Davisberg|             Belgium|ronaldrusso@examp...|  472-927-7368x62109|\n",
      "|       2319|       Kevin Powell|6839 Jennifer Street|    New Evanview|              Gambia|muelleradam@examp...|001-809-857-0128x...|\n",
      "|       2322|      Beth Johnston|8740 Ashley Haven...|   West Samantha|       French Guiana| umartin@example.net|001-581-812-7932x...|\n",
      "|       2478|  Ronnie Richardson|   950 Nicole Drives|     Mercadofurt|               India|patrick95@example...|    603-995-8579x561|\n",
      "|       2699|            Unknown|43697 Sonia Exten...|     South Amber|        South Africa| xhanson@example.com|   623-817-7190x7549|\n",
      "|       2978|  Christopher Irwin|     9415 Wells Well|         Unknown|             Liberia|  crojas@example.org|  (804)819-1580x8410|\n",
      "|       3126|  Christopher Davis|6519 Dwayne Pine ...|     North Tracy|    Marshall Islands| becky72@example.com|001-932-786-9899x...|\n",
      "|       3239|      Heidi Vazquez|5346 Gordon Shore...|      Briggsberg|             Unknown| pobrien@example.org|          3294797221|\n",
      "|       3525|       Kenneth Hill|3667 Buchanan Tra...|         Unknown|    French Polynesia|rebeccamathis@exa...|001-726-760-4047x318|\n",
      "|       3577|    Megan Hernandez|    15096 Olson Cape|      Nguyentown|             Belgium|cmiddleton@exampl...|     +1-884-568-2209|\n",
      "+-----------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employee schema\n",
    "Employee =nuga_bank.select('Company', 'Job_Title', 'Gender','Marital_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding an employeeID column\n",
    "Employee = Employee.withColumn('Employee_ID', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RE-arranging the columns\n",
    "Employee = Employee.select('Employee_ID', 'Company', 'Job_Title', 'Gender','Marital_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "|Employee_ID|             Company|           Job_Title| Gender|Marital_Status|\n",
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "|          0|Benson, Johnson a...|             Unknown|  Other|      Divorced|\n",
      "|          1|             Unknown|   Food technologist| Female|       Married|\n",
      "|          2|       Jones-Mueller|Database administ...|  Other|       Unknown|\n",
      "|          3|       Vargas-Harris|Horticultural the...|Unknown|       Unknown|\n",
      "|          4|Richardson, Gonza...|   Minerals surveyor| Female|       Married|\n",
      "|          5|           Smith Ltd| Seismic interpreter|  Other|       Married|\n",
      "|          6|         Wade-Kelley|  Surveyor, minerals|   Male|       Unknown|\n",
      "|          7|             Unknown|Medical laborator...| Female|        Single|\n",
      "|          8|         Lindsey LLC|Programmer, appli...| Female|        Single|\n",
      "|          9|         Carroll LLC|             Unknown|   Male|        Single|\n",
      "|         10|             Unknown|     Sales executive|   Male|      Divorced|\n",
      "|         11|Lynch, Hayes and ...|       Stage manager|Unknown|       Married|\n",
      "|         12|           Green Ltd|Psychologist, pri...|   Male|       Unknown|\n",
      "|         13|             Unknown|Designer, blown g...| Female|        Single|\n",
      "|         14|     Morris-Reynolds|          Podiatrist|  Other|       Married|\n",
      "|         15|         Sutton-Owen|   Financial planner|  Other|       Married|\n",
      "|         16|         Brown Group|Nutritional thera...|  Other|       Married|\n",
      "|         17|         Scott Group|     Games developer|  Other|       Married|\n",
      "|         18|Bryant, Huffman a...|Research scientis...|  Other|       Married|\n",
      "|         19|          Palmer PLC|             Unknown|   Male|      Divorced|\n",
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fact schema\n",
    "Customer_Facts = nuga_bank.join(Customer, ['Customer_Name', 'Customer_Address', 'Customer_City', 'Customer_Country', 'Email', 'Phone_Number'], 'left')\\\n",
    "                            .join(Transaction, ['Transaction_Date', 'Amount', 'Transaction_Type'], 'left')\\\n",
    "                            .join(Employee, ['Company', 'Job_Title', 'Gender','Marital_Status'], 'left')\\\n",
    "                            .select('Transaction_ID', 'Credit_Card_Number', 'IBAN', 'Currency_Code', 'Random_Number','Category', 'Group','Is_Active', 'Last_Updated','Description')\\\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "|Transaction_ID|Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|Group|Is_Active|        Last_Updated|         Description|\n",
      "+--------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "|   17179869198|   180067592769732|GB92JVMY004197871...|          EGP|       7198.0|       A|    Z|      Yes|2023-10-12 22:25:...|Before story prof...|\n",
      "|            18|   213112163828334|GB50TJFN039979307...|          SVC|       7382.0|       B|    Z|      Yes|2020-01-19 18:19:...|Great evening so ...|\n",
      "|   25769803779|                 0|GB32LGFL895760023...|          PAB|       8898.0|       A|    Y|      Yes|2021-12-07 15:35:...|Face field coach ...|\n",
      "|   17179869199|  4239162655922295|GB96MEEY268453596...|          BTN|       5605.0|       D|    Z|       No|2020-03-22 00:53:...|Visit present reg...|\n",
      "|    8589934596|     4021800082481|GB78UBAH195883770...|          LBP|       8097.0|       C|    X|      Yes|2023-07-11 06:06:...|Maybe teacher dee...|\n",
      "|    8589934592|   213133896337542|GB07IUUE487965913...|          ISK|          0.0|       D|    Y|       No|2023-08-16 10:32:...|Themselves make ago.|\n",
      "|            20|   213186454811670|GB77SPMZ984195063...|          SOS|       7610.0|       C|    X|       No|2021-04-15 13:30:...|Concern remember ...|\n",
      "|    8589934601|  6011177559558424|GB76DNER497098023...|          BZD|       6499.0|       D|    X|  Unknown|2020-02-09 13:11:...|Risk take recent ...|\n",
      "|   25769803790|  6574717409681808|GB58ZWIH992989778...|          LSL|          0.0|       B|    Y|       No|2021-11-04 10:41:...|Any state recogni...|\n",
      "|   17179869184|    38737697224239|GB44WJZF814413539...|          JMD|       8569.0|       C|    Z|  Unknown|2021-01-25 12:59:...|             Unknown|\n",
      "|             8|   180039947294310|GB48WNWB013807482...|          HRK|       3061.0|       B|    X|       No|2020-08-31 20:40:...|Notice paper son ...|\n",
      "|            11|  4304956118653227|GB15VYWJ018711823...|          NIO|       2502.0|       B|    Y|      Yes|2021-04-12 20:23:...|Rest theory serve...|\n",
      "|    8589934605|   213123497507940|GB61RQDC156293863...|          GBP|          0.0|       B|    Z|      Yes|2023-01-29 06:25:...|    Plant very name.|\n",
      "|   25769803783|    30472377458372|GB78IXJB107495171...|          MDL|       6463.0|       A|    Z|       No|2020-09-25 02:11:...|Country stage sam...|\n",
      "|   17179869197|      639006216746|GB81CQQR004909975...|      Unknown|       8199.0|       B|    Y|       No|2024-03-31 18:51:...|Try step sound. B...|\n",
      "|   17179869204|     4559268055614|             Unknown|          BBD|       5380.0|       D|    Y|       No|2024-04-16 14:15:...|Central improve q...|\n",
      "|    8589934610|                 0|GB79VCXM025662145...|          SPL|       7594.0|       D|    X|      Yes|2021-06-26 14:31:...|Fall report stay ...|\n",
      "|             7|     4500718397537|GB78FDAJ595830659...|          RON|       2445.0|       B|    X|       No|2022-11-19 08:59:...|             Unknown|\n",
      "|             7|     4500718397537|GB78FDAJ595830659...|          RON|       2445.0|       B|    X|       No|2022-11-19 08:59:...|             Unknown|\n",
      "|             7|     4500718397537|GB78FDAJ595830659...|          RON|       2445.0|       B|    X|       No|2022-11-19 08:59:...|             Unknown|\n",
      "+--------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Customer_Facts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o653.parquet.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#saving data to parquet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mTransaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTransaction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o653.parquet.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "#saving data to parquet\n",
    "Transaction.write.mode('overwrite').parquet(r'dataset\\Transaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving data to csv\n",
    "Transaction.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset\\transformeddata\\csv\\transaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#convert spark dataframe to panda dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# transaction_df = Transaction.toPandas()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# customer_df = Customer.toPandas()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# employee_df = Employee.toPandas()\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m facts_df \u001b[38;5;241m=\u001b[39m \u001b[43mCustomer_Facts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\sql\\pandas\\conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     90\u001b[0m jconf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jconf\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\sql\\pandas\\utils.py:24\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m minimum_pandas_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "#convert spark dataframe to panda dataframe\n",
    "\n",
    "transaction_df = Transaction.toPandas()\n",
    "customer_df = Customer.toPandas()\n",
    "employee_df = Employee.toPandas()\n",
    "facts_df = Customer_Facts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset into a postgresql DB\n",
    "db_Params ={\n",
    "    'username':'postgres',\n",
    "    'password':'root',\n",
    "    'host':'localhost',\n",
    "    'port':'5432',\n",
    "    'database': 'nuga_bank'\n",
    "}\n",
    "\n",
    "#defining the dabase connections\n",
    "db_url = f\"postgresql://{db_Params['username']}:{db_Params['password']}@{db_Params['host']}:{db_Params['port']}/{db_Params['database']}\"\n",
    "\n",
    "#create the database engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "#connect to the postgresSQL server\n",
    "with engine.connect()as connection:\n",
    "    transaction_df.to_sql('Transaction',connection, index=False, if_exist='replace')\n",
    "    customer_df.to_sql('Customer',connection, index=False, if_exist='replace')\n",
    "    employee_df.to_sql('Employee',connection, index=False, if_exist='replace')\n",
    "    facts_df.to_sql('Customer_Facts',connection, index=False, if_exist='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.5/1.2 MB 932.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.0/1.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.0/1.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
